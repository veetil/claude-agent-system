Great, I’ll conduct deep research into designing a Claude Code–based multi-agent system that meets your performance, coordination, and outcome-based criteria.

I’ll focus on:

* Feasibility with Claude’s system limitations (including the 800 prompt/5-hour limit)
* Outcome-first architectures that minimize drift
* Memory strategies: what to share, when, and where (live context vs. database vs. vector store)
* Recursive agent tasking models with blocking and non-blocking flows
* Making research a first-class capability within the agent system

I’ll return with a comprehensive report shortly.


# Designing a Multi-Agent System with Claude Code

## Claude Code Constraints and Model Selection

**Claude Code Environment:** Claude Code is Anthropic’s CLI tool for “agentic coding” – essentially a command-line interface that lets you interact with Claude models in a flexible, scriptable way. When designing a multi-agent system with Claude Code, we must respect the platform’s constraints: notably, **usage limits** of roughly **200–800 Claude Code prompts per 5-hour session** under the Max 20× plan. This prompt quota means our system should be efficient in its exchanges – packing multiple operations into one prompt when possible and avoiding needless back-and-forth. The CLI supports command-line invocation and can integrate with shell commands and tools via Anthropic’s Model Context Protocol (MCP), which we can leverage for agent tool use (e.g. web browsing or code execution).

**Model Choices:** Anthropic offers several Claude model variants, each with different capabilities suited to agents: **Claude Opus 4**, **Claude Sonnet 4**, and **Claude 3.5 Haiku**. All Claude 4 models support a *200k token context window* – excellent for holding extensive multi-agent dialogues or large knowledge bases in memory. **Opus 4** is the flagship, offering the best coding and reasoning performance, and can sustain *hours of focused operation across thousands of steps* without losing coherence. It’s ideal for long-running, complex agent workflows and heavy-duty reasoning. **Sonnet 4** is a strong general-purpose model that balances capability with efficiency (it’s available to all users). It excels at coding and reasoning with high steerability and even supports *up to 64K tokens of output* generation for verbose tasks. **Haiku 3.5**, while a previous-gen model, is *Anthropic’s fastest and most affordable model*, making it useful for lightweight or high-volume sub-tasks. In fact, Claude 3.5 Haiku’s speed and improved tool use make it *well suited for specialized sub-agent tasks* where low latency is critical. In our design, we can **mix models** as needed: e.g. use Opus 4 for a “planner” agent that requires deep reasoning, Sonnet 4 for interactive agents or user-facing assistants (it has low hallucination rates, ideal for Q\&A on large knowledge bases), and Haiku 3.5 for utility agents that need quick responses. Each model runs through the same CLI, so the orchestration can select a model per agent based on the task’s complexity and required speed. We also note that Opus 4 and Sonnet 4 have a “hybrid reasoning” mode: they can produce *near-instant replies or enter extended thinking* when asked to “think harder,” allowing dynamic trade-off between speed and thoroughness. We will leverage this to keep prompts efficient – e.g. having a single prompt trigger extended reasoning and tool use internally, rather than multiple short prompts.

Finally, Claude Code’s design is **unopinionated and low-level**, giving us raw access to the model’s capabilities. We are free to script our own agent loops and coordination logic without an imposed framework, which suits our custom-built approach (and aligns with avoiding external orchestration frameworks like LangGraph or Autogen). We will ensure any potentially destructive actions by agents (file writes, shell commands) are handled via Claude Code’s permission system or in safe sandbox environments.

## Outcome-Driven Architecture (Quality over Quantity)

This multi-agent system will be **strictly outcome-driven**: every architectural choice prioritizes the *quality and accuracy of the final output* over the number of agents or fanciness of interactions. Multi-agent setups can suffer “output drift” – as agents pass information around, the final answer can veer off-topic or accumulate errors. To combat this, our architecture uses a **plan-and-verify approach** at each stage. We introduce a top-level *Orchestrator* (or “manager”) agent that keeps the **global goal** in focus and monitors intermediate results for accuracy, acting as a safety net against hallucinations. For example, if a Research agent fetches facts, the Orchestrator (or a separate Reviewer agent) will cross-check those facts (possibly by asking for sources or using a second search) before they are used in the final answer. This kind of **cross-validation mechanism** – where one agent’s output is reviewed by another – helps catch errors before they propagate. Anthropic’s own best-practice suggests using *independent subagents to verify details*, especially for critical steps, which we adopt to maintain accuracy.

Crucially, we will **minimize the number of dialogue turns and agents** involved unless they demonstrably improve results. Each additional agent-agent communication is a chance for misunderstanding or compounding hallucination, so we avoid needless chatter. Instead, agents should produce well-structured outputs that can be directly consumed by the next stage. For instance, the Research agent should output a concise factual report with citations, rather than a long narrative that a subsequent agent might misinterpret. We also prefer *specialized tool use over LLM-to-LLM chat* whenever possible – e.g. using Claude’s built-in web search tool to retrieve facts with citations, instead of having one agent “tell” another agent something that might get distorted. Each agent’s role and output format will be clearly defined to reduce ambiguity.

To further preserve quality, we exploit Claude models’ **advanced reasoning features**. We can prompt agents with directives like “think harder” or even “ultrathink” to have Claude devote more internal computation to a complex problem. This extended reasoning happens within a single prompt (Claude will internally perform more analysis or multi-step tool use in one turn) and can replace what might otherwise require multiple simpler prompts. By doing so, we reduce the total number of prompts (staying under quotas) and also keep the reasoning self-contained, which limits drift. In essence, if one agent can solve a sub-problem in a single well-crafted prompt (perhaps by internally looping through tool usage), we won’t split that task into multiple agents. The **simplest effective agent topology** is chosen for each problem.

Notably, Anthropic reports that Claude 4 models have been tuned to resist “shortcuts and loopholes” in agentic tasks – they are *65% less likely to cheat or hallucinate a quick answer* instead of doing the proper reasoning. This improved reliability is a boon for outcome quality. Opus 4 in particular has stronger long-horizon focus and will stick to a task for hours without derailing, which helps maintain consistent output in lengthy workflows. We will harness these capabilities by using Opus 4 for the most complex, lengthy reasoning chains and Sonnet 4 for tasks requiring careful instruction-following and low hallucinations (like final answer generation).

In summary, the architecture will keep agents on-task through clear objectives and verification. A *single orchestrator or lead agent* will maintain the big picture and sanity-check outputs. All agents will share a common goal state (provided via system prompts or the Orchestrator’s reminders) so that even as they operate independently, their contributions align towards the final desired result. By focusing on final output correctness and coherence – even at the cost of a few extra verification steps – we ensure the multi-agent system truly improves quality rather than degrading it.

## Flexible Agent Topologies Adapted to Tasks

Rather than a one-size-fits-all pipeline, our system will support **multiple agent architectures** and choose the configuration that best fits each task. This flexibility is key to keeping the system outcome-driven. We’ll design a **modular architecture** where agents are like interchangeable specialists that can be assembled in different patterns (sequence, branches, hierarchy, etc.) depending on the problem.

For example, for a straightforward coding task, we might not need multiple agents at all – a single Claude Code agent using the built-in bash and code tools could suffice. But for a complex research-heavy task (say, writing a market analysis report), we could employ a *“dream team” of specialized LLM agents*: one agent dedicated to gathering data, another to analyze and draw insights, and a final one to synthesize the output. This specialization by role ensures each agent focuses on what it does best. The system’s orchestrator can spin up these agents and route tasks to them as needed. In essence, the architecture is **dynamic** – the orchestrator first assesses the user’s request and then instantiates an appropriate set of agents and a communication topology among them.

Several topology patterns will be supported:

* **Linear pipeline:** Agents act in sequence, each transforming the output of the previous. Example: *Research Agent* → *Drafting Agent* → *Editing Agent*. This is useful when each stage clearly feeds into the next. We’ll use this for tasks like document creation (research -> write draft -> refine draft).

* **Manager-worker hierarchy:** A top-level *Planner Agent* decomposes the task and delegates sub-tasks to various *Worker Agents*, then aggregates the results. This pattern is powerful for complex, multi-part tasks. For instance, a Planner might break a software project into coding, testing, and documentation subtasks assigned to different Claude Code agents, then integrate their outputs. The Planner (manager) can run either synchronously (waiting for each worker’s result in turn) or asynchronously (dispatch all sub-tasks in parallel threads and then join results). This **hierarchical delegation** mirrors what frameworks like AutoGen allow (scripting arbitrary agent interaction graphs), but we’ll implement it custom with Claude Code’s SDK. The GPT-Newspaper project is an illustrative example of this pattern: it uses a *planner agent* to generate research questions and an *execution agent* to find information for each question, then the planner consolidates the findings into a report. We intend to follow a similar approach for research-centric tasks – one agent plans what to research, multiple queries are executed (possibly in parallel) by a research agent or multiple instances of it, and the planner assembles the final answer.

* **Round-table (cooperative)**: In some cases, we might let multiple agents discuss or debate to reach a better answer (e.g., a “devil’s advocate” agent questioning an answer). This is less frequently used (because it consumes more prompts and risks drift), but for critical decisions or creative brainstorming it can improve results. If invoked, we ensure a strict protocol: e.g., each agent states a proposal, they critique each other once, then a moderator agent (or the orchestrator) decides. This controlled debate can harness multiple perspectives while limiting endless loops. Some research has shown that fusing diverse reasoning paths from multiple LLM “agents” can outperform single models on certain problems, so having the option for agents to run in parallel and then merging their answers (using a voting or MCTS-like approach) is beneficial for, say, hard logical or math problems (as in the MoSA method). Our design will allow such parallel solving with a merge step when needed.

The key is that the **system can reconfigure on the fly**. We might implement this via a central Orchestrator script (Python or Node) that examines the task description and selects a “strategy template.” For instance, if the task type is “research and write,” use the Planner/Researcher/Writer trio; if it’s “code and test,” use a Code Agent and a Test Agent in iterative loop; if it’s simple Q\&A, maybe just one agent with a search tool. This adaptability ensures we aren’t running a whole swarm of agents for a task that one agent could do (avoiding unnecessary complexity), and conversely that we bring in multiple agents when the single-agent approach might falter or be inefficient.

Implementationally, we will maintain a library of agent classes or prompt templates corresponding to roles (e.g., a **ResearchAgent** class, **CodingAgent**, **PlanningAgent**, etc.), and the Orchestrator composes them as needed. Communication channels between agents will likewise be flexible: by default through the orchestrator (one agent’s output is passed as input to the next), but in complex cases possibly via a shared memory or file if multiple agents need to access the same data. Because Claude Code now offers a background SDK for running agents programmatically, we can integrate this into Node.js or Python – spawning agent threads and controlling their interactions in code. The design avoids hard-coding a single workflow and instead focuses on an **architecture that can plug in different agent topologies** depending on the end goal.

## Coordination and Memory Management

Effective multi-agent performance requires robust **coordination mechanisms** and thoughtful **memory architecture** so that agents remain in sync and relevant information persists throughout the session.

### Short-Term vs. Long-Term Memory

Each Claude agent has a huge context window (especially Opus/Sonnet 4 with 200K tokens) for **short-term memory**, meaning it can recall a large conversation history or documents provided in the prompt. We will utilize this by feeding agents the necessary recent context (e.g. the Planner will receive summaries of each Worker’s result when integrating). However, over a multi-hour session with potentially hundreds of prompts, even 200K tokens can eventually fill up. To avoid context overflow and forgetting earlier details, we implement a **long-term memory** mechanism external to the immediate prompt. Recent research and tools show that giving LLM agents persistent memory significantly enhances their long-horizon reasoning. Anthropic’s Claude models themselves support a concept of *“memory files”* – when given local file access, Claude can write notes to disk and later read them to maintain continuity and tacit knowledge. In fact, Claude Opus 4 will autonomously create and update such memory files (for example, logging important facts as it plays a game) to improve long-term task awareness. We will mirror this approach: agents in our system can explicitly write key facts, interim conclusions, or decisions to a **shared memory store**. This store could simply be a local file or database that all agents have access to (Claude Code can read files or call a database via tools). For more structured memory, we might use a *vector database* (embedding store) as a long-term knowledge base: agents can embed text (e.g., a summary of what’s been learned so far) and upsert it into a vector DB, and later agents can query by embedding their question to retrieve relevant past info (this is a classic Retrieval-Augmented Generation approach). Whether file or vector DB (or both), the idea is to maintain a **persistent context** that does not rely on the prompt token window alone.

**Short-term memory** will be managed by techniques like windowed context and summarization. For instance, if an agent has a very long chain-of-thought, we can periodically summarize earlier turns and replace them with the summary (Anthropic even notes that Claude 4 will auto-summarize lengthy thoughts \~5% of the time using a smaller model). Our orchestrator can similarly prompt an agent to summarize (“recap what the plan is so far”) and then store that recap externally. We will also use **session state files** (or Claude’s `CLAUDE.md` mechanism) to hold important context like project instructions or core assumptions, which Claude Code automatically pulls into context each session. In short, short-term memory (prompt history) will be pruned and augmented by summaries as needed, and long-term memory will reside in external storage accessible throughout the 5-hour session (and even across sessions if needed for persistent projects).

### Knowledge Sharing Between Agents

A central design question is **what information agents should share, when, and how**. Sharing too little leads to each agent working in isolation (possibly duplicating work or missing global context), whereas sharing too much (e.g. dumping one agent’s entire dialogue into another’s prompt) can overwhelm context or propagate errors. We strike a balance with a **two-tier memory** approach, inspired by recent research on multi-agent memory sharing: each agent has its own private working memory (its internal chain-of-thought and any private data), and there is a **shared memory** for information that needs to be common knowledge (like the overall task goal, or facts uncovered by the Research agent that everyone should use). The Orchestrator enforces what goes into shared memory. For example, after the Research agent completes, the Orchestrator might take the top relevant facts it found and push those into the shared store (and perhaps also into each subsequent agent’s prompt as context). Less relevant details or the raw search transcripts might be omitted to avoid clutter. This corresponds to a *“write policy”* where only certain distilled fragments are shared. Similarly, the Orchestrator can control each agent’s “read access” – e.g., the Writing agent may be allowed to retrieve all facts from shared memory, but a code-generation agent might only need the specific API endpoints found earlier. By implementing simple filters or tags on memory entries, we can prevent agents from being confused by data irrelevant (or unauthorized) to their role.

Concretely, **real-time vs. post-task sharing** will be handled as follows: We avoid constant real-time chatter between agents (which would burn through the prompt quota). Instead, we favor a **post-task aggregation**: each agent, on completing its subtask, produces an output artifact (a summary, a file, results, etc.) that gets stored and potentially fed into the next phase. For instance, the Research agent’s summary report (in shared memory) will later be injected into the Writing agent’s prompt. However, in cases where multiple agents truly need to work simultaneously (say two agents performing different searches), we will allow them to *update the shared memory in parallel* and even read each other’s contributions in real-time. Anthropic’s tooling supports connecting Claude Code to external MCP servers or even running multiple Claude instances, so parallel writes to a common memory (like a shared file or DB) are feasible. In essence, our system can function like a blackboard architecture: agents post results to a “blackboard” (shared memory), and other agents can read and contribute as needed. This ensures that, for example, if Agent A discovers a crucial piece of information, Agent B can immediately incorporate it instead of reinventing it. We just must guard against one agent’s errors polluting others – hence the earlier verification steps and careful curation of what gets written as truth.

**Storage choices:** We have multiple options for implementing memory storage:

* Using the **Claude Files API** (recently introduced) to store data within Claude’s session. This would let Claude store and retrieve text files up to certain sizes, effectively giving it a scratchpad on Anthropic’s side.
* Using **local files** in the environment (Claude Code can read/write to local files, given permission, as seen with memory files in Opus 4). This is straightforward and leverages the CLI’s ability to handle filesystem.
* Using an **embedded Vector DB** accessible via API calls (for example, a simple Python-based FAISS index or a cloud vector store) – the agents could call this through a custom tool. If we go this route, we’d have a dedicated “Memory Agent” or just the Orchestrator performing the vector queries and feeding results to agents.

Regardless of implementation, the memory architecture will enable *knowledge reuse* and *consistency*: Agents won’t repeatedly ask the same question if the answer was already found – they’ll check memory first. If an agent comes up with a plan or assumption, it records it so that others can align to the same plan. This addresses one of the major challenges noted in multi-agent systems: managing context and keeping everyone on the same page. Our approach ensures that even if agents are loosely coupled, they all refer back to a coherent knowledge source, reducing divergence.

## Recursive Task Delegation and Hierarchical Planning

The system supports **recursive task decomposition**, meaning an agent can break down a complex goal into sub-goals and invoke either new agents or even additional instances of itself to solve those sub-goals. This is particularly useful for problems that naturally split into parts or require multi-step reasoning.

Our design will include a **Planner Agent** (or use the Orchestrator in that capacity) that is capable of analyzing a task and formulating a plan of action. This plan will list sub-tasks which might be executed serially or in parallel. We will implement recursion carefully to avoid runaway loops: typically, the planner will only recurse one level at a time (it creates sub-tasks and waits for their completion). If a sub-task itself is very complex, the planner could in theory spawn a sub-planner for it, but in practice we might handle multi-level breakdown by having the main planner simply produce a hierarchical plan (like a tree of tasks) at once and then work through the tree.

For **blocking (synchronous) delegation**: the Planner agent assigns a task to a worker and *pauses* until the worker returns a result. This ensures the planner has all necessary information before proceeding to the next step, which maintains order and makes reasoning easier. We’ll use blocking mode when tasks have dependencies or must be done in sequence. For instance, “first retrieve relevant research, then draft the report” – drafting must wait until research is done.

For **non-blocking (parallel) delegation**: the Planner can dispatch multiple tasks at once and later gather their results. Claude Code’s CLI can be run in the background (Anthropic even mentions running Claude Code in parallel/background for long tasks), and our orchestrator code (Python/Node) can spawn multiple async tasks calling the Claude API concurrently. We’ll use this for independent sub-tasks to save time – e.g., if three different pieces of information need to be researched, spin up three research agent calls concurrently. We must then have a mechanism to aggregate results – the Planner will have to wait for all to finish or be notified as each completes. This is straightforward to implement with async/await patterns or thread pools in Python.

One important aspect of recursion is **result integration**. After sub-agents finish, the higher-level agent (Planner) needs to combine their outputs. We’ll equip the Planner with logic to verify and integrate results: it may cross-check consistency between sub-results or handle contradictions. If subtask A and B produce conflicting info, the Planner can trigger a follow-up resolution (perhaps asking a third agent to fact-check which is correct, or returning to the research agent for clarification). This hierarchical approach acts as a funnel: multiple parallel explorations feed into one coherent answer.

We will also incorporate **hierarchical role definitions**: for instance, a Planner can be instructed (via its prompt) to only decide *what* to do, not *how*. The actual doing is delegated. This aligns with the principle of separation of concerns. A concrete mechanism might be that the Planner’s output is a list of “actions” in a structured format (like a JSON or markdown checklist). The Orchestrator script reads that and for each action, calls the appropriate agent. This way, Claude (Planner) is effectively programming its own sub-agents via our orchestrator – a form of self-delegation. Such patterns have precedent (e.g., giving an LLM a scratchpad to output tool commands which the system then executes, as in the ReAct paradigm). Here, the “tools” are other agents. For example, the Planner might output: “1. (ResearchAgent) Search for: latest market growth data. 2. (AnalysisAgent) Analyze how this data impacts the client’s product.” The orchestrator sees this and knows to invoke the ResearchAgent with that query, then pass the result to an AnalysisAgent with the instruction. This design ensures **recursion is controlled** – the Planner can’t spawn infinite agents arbitrarily; it must output a finite plan that we execute step by step. We can impose a safeguard like: if an agent’s plan includes calling another planner recursively beyond a certain depth, we stop or have a human review. In practice, most tasks will likely only need one level of delegation (maybe two at most), which is manageable.

By enabling recursion and hierarchical delegation, we ensure the system can **tackle complex, composite tasks** in a structured way. As a real-world analogy, think of a project manager (Planner agent) breaking a project into tasks and assigning them to team members (worker agents), then assembling the final deliverable – our system mimics this pattern. This helps avoid the common pitfall of a single agent attempting to do everything in one prompt (which often fails for very complex tasks) while still keeping agents aligned under a top-level strategy. It also resonates with human workflows, making the system’s process more transparent and debuggable (we can inspect the planner’s plan and each sub-result easily).

## Research-Centric Design with Reusable Agents and Tools

Nearly all non-trivial tasks benefit from current information or external knowledge, so our multi-agent system places heavy emphasis on **research capabilities**. We will create a dedicated **Research Agent** role that can be invoked whenever the task requires information beyond the agents’ initial knowledge. This Research agent will be built to be **reusable, reliable, and efficient**:

* **Reusable:** Instead of coding research logic separately for each type of task, we’ll have one robust ResearchAgent that any other agent can call (via the orchestrator) with a query. For example, both a writing task and a coding task might need background info – they’ll both delegate to the same ResearchAgent with appropriate prompts. This agent will use Claude’s tool-use abilities to perform web searches and return a summary of results. Claude 4 models have a beta **Web Search tool** that allows them to issue real-time search queries and read the results, even citing sources automatically in the answer. We will leverage this when available – essentially giving the ResearchAgent a direct internet access skill. Alternatively or additionally, we can integrate external search APIs (like the SerpAPI for Google search or Perplexity.ai’s API) as MCP tools. Claude Code supports connecting to custom MCP servers/tools defined in a config (for instance, adding a Puppeteer browsing tool as shown in Anthropic’s docs). We can register a **“web\_search” tool** in the Claude Code environment (either the built-in one or a custom connector) so that when the ResearchAgent is prompted to find information, it can call out to the web safely.

* **Reliable:** To ensure the ResearchAgent doesn’t introduce hallucinations, we design its prompt template to **prioritize factual accuracy and evidence**. It should return answers with citations or direct references whenever possible. Since the Claude web search tool already cites sources, we will use that feature – the ResearchAgent’s output could be a brief report like: “According to Source X, ... (cite URL). Source Y states ....” If we roll our own solution via an API, we will program the agent to quote snippets from results rather than making up content. We might also have the ResearchAgent retrieve multiple sources and either consolidate them or pass them to another agent for synthesis, to reduce reliance on a single potentially wrong source. Furthermore, as an extra reliability measure, we could have a second agent verify critical facts (this loops back to the cross-checking strategy from outcome-driven design). For example, if the ResearchAgent returns a key statistic, the Orchestrator could quietly ask the ResearchAgent (or another instance of it) a second time to double-check that stat via a different query or source. Only if it corroborates do we accept it into shared memory for use by others.

* **Efficient:** Efficiency in research comes from not doing redundant work and not over-searching. We’ll implement caching at multiple levels. First, our system will maintain a cache of recent queries and their results – if a new query is very similar to one already asked, we can reuse the previous answer (perhaps after a quick validation that nothing has changed if it’s time-sensitive). Anthropic’s prompt caching (the API can cache prompt responses for up to an hour) could be useful here. For example, if multiple sub-agents need the same background info, the first query’s result can be stored and subsequent agents can be given that info instead of each doing their own web search. We’ll also encourage the ResearchAgent to perform **batched queries**: if it knows it needs to find several facts, it can issue them in a single extended thinking session or use multi-turn search within one prompt (Claude can do multiple search queries in one tool-using prompt turn). This reduces the number of separate prompts used. Additionally, by making the ResearchAgent focus only on research, we free other agents from spending tokens describing how to do research – they can simply request data, and the specialist handles it. This specialization aligns with the multi-agent benefit of dividing tasks by expertise.

From an implementation standpoint, integrating research could be done via the Claude Code MCP system. We might run a local MCP server that wraps the Perplexity API or Google’s SERP, and list it in the `.mcp.json` config so Claude knows it has that tool. Then, in the ResearchAgent’s prompt, we’d instruct: “Use the `web_search` tool to find X, and summarize the findings with sources.” Given Claude’s capabilities, it will autonomously decide when and how many searches to run, and then output a summary. Our orchestrator will ensure the agent has permission to use that tool (managing the allowlist as needed).

Finally, we treat research results as first-class artifacts in the system. They will typically be written to the shared memory (with source references). Other agents, like a writing agent, might even incorporate the citations into the final output for transparency. And because the research agent is reusable, we could even expose it directly to the end user for on-demand factual queries (like an ask-the-expert function), knowing it’s governed by the same reliability measures.

In summary, a **robust research subsystem** is central to our multi-agent architecture. By making it modular and focusing on accuracy, we mitigate one of the biggest weaknesses of agentic systems – the tendency to go off-track due to lack of knowledge. Any task that requires outside info will automatically invoke this subsystem, yielding informed agents that ground their work in reality rather than hallucination.

## Prompt Usage Tracking and Quota Management

Given the strict **prompt limit (800 per 5 hours)** in Claude Code, our system will include mechanisms to actively monitor and optimize prompt usage. We will maintain a **global counter** of prompts used in the current 5-hour session. Each time the orchestrator sends a message to an agent (or the agent uses a tool in a way that counts as a prompt), we increment the counter. Claude Code does provide warnings as you approach the limit, but we won’t rely solely on that – our own tracking will allow us to proactively adjust behavior. For example, if we see we’re nearing the quota (say >750 prompts used), the orchestrator can decide to **throttle or batch further requests**. Non-urgent tasks can be queued to run after the session reset (5-hour window reset) if needed. If a user triggers a very large job that would exceed the quota, the system can warn or ask for confirmation to proceed in parts.

We will also exploit **prompt-efficient strategies** to maximize utility from each prompt:

* **Batching queries:** Wherever possible, combine multiple questions or commands into one prompt. Claude’s large context means an agent can handle multi-part instructions in one go. For instance, rather than prompting the code agent file-by-file, we could ask it to modify several functions across files in one prompt (if context size permits). The Best Practices guide suggests batching related queries and reading multiple files at once to save prompts.
* **Extended thinking mode:** As discussed, using Claude’s extended reasoning within a single prompt can do the work of many smaller prompts. This is essentially trading compute for fewer interactions. We can programmatically add “think hard” or increase the `max_chain_length` (if using the API) to let Claude solve a sub-problem internally instead of iterative prompting.
* **Caching and reuse:** Anthropic mentions a **Projects** feature where uploading documents once doesn’t incur token cost on subsequent uses in that project. We will organize tasks into project sessions so that, for example, if multiple agents need access to the same large document, we only provide it once and then refer to it. Additionally, as noted, the prompt caching feature (caching model responses for an hour) means if we anticipate calling the same prompt again, we can reuse results. This is relevant if our orchestrator has to retry an action or if two users ask identical questions in a short span (in a multi-user scenario).
* **Graceful degradation:** If the quota is hit unexpectedly, the system will catch the rate-limit error and handle it: possibly by falling back to a simpler mode (e.g., switch to a local smaller model if available for interim results) or by pausing and resuming after the cooldown. Since the limit resets every 5 hours per Anthropic’s policy, our system can schedule a job to continue where it left off once the window resets (persisting necessary state to disk in the meantime). This ensures long jobs eventually complete.

By incorporating these tactics, we aim to **never outright violate the 800 prompt limit**. Instead, the system will be self-aware of its usage. We will also provide usage feedback to the user or developers – e.g., a running counter or alerts like “Research phase used 50 prompts, X remaining”. This helps with transparency and planning work around the 5-hour sessions.

In essence, prompt budgeting becomes another aspect of the orchestrator’s duties. The orchestrator will function not just as a router but as a *governor*, enforcing a budget on agents. If an agent is in a loop that is consuming too many prompts (say a problematic chain of thought), the orchestrator can step in, possibly terminating that agent and summarizing progress so far to restart more efficiently. This prevents runaway prompt usage, aligning with our outcome-driven ethos (we want high quality output with minimal overhead).

## Implementation Notes and Best Practices

Finally, a few notes on tools and libraries for implementing this system in **Python, Node.js/Next.js, and Bash** (the user’s preferred stack):

* **Claude API/SDK:** We recommend using Anthropic’s official API to drive Claude models, which can be accessed from Python or Node. Anthropic provides a Python SDK (and possibly a Node SDK) for Claude; using these will simplify prompt calls. For example, in Python you can use the `anthropic` library to create a client and send prompts, specifying the model (Opus/Sonnet/Haiku) and tools (like web\_search). In Node/Next.js, you could either call the REST API directly via `fetch/axios` or use any available wrapper package. Since Claude Code is a CLI, another approach is to script the CLI via subprocess calls (from a Node child\_process or a Bash script). However, using the API/SDK gives more control in a server environment (Next.js backend).

* **Parallelism and Orchestration:** In Python, we can use `asyncio` or threading to run multiple agent calls concurrently for non-blocking sub-tasks. In Node.js, the naturally async model works well – use `Promise.all` for parallel calls to Claude’s API when needed. The orchestrator logic – deciding agent roles and coordinating – can be written in either Python or Node depending on integration needs (Python might have the edge for easy integration with AI libraries, Node might integrate better with a Next.js frontend). Even Bash can orchestrate simple sequences (shell scripts calling `claude` CLI for each agent in turn), but for complex logic and parallelism, a higher-level language is preferable. Bash could still be used to launch background Claude Code sessions or schedule tasks (e.g., using cron to resume after a delay).

* **Memory Storage:** For a quick implementation, using the filesystem is simplest – e.g., a shared directory where agents write JSON or Markdown files with their outputs. Each agent (Claude instance) can be given access to read/write those files (by whitelisting `Edit` and file paths in Claude Code permissions or by using the Files API to have them read the content). Alternatively, for a more robust solution, you can integrate a database or vector store. Python has libraries like **FAISS** or **ChromaDB** for in-memory vector DB, and Node could connect to an external vector DB service via REST. The memory management logic (private vs shared) can be implemented on top of whichever storage by simply using different file namespaces or DB collections for private vs shared memory.

* **Tool Integration:** We will enable the **Web Search tool** (Anthropic’s or custom) as described earlier. Additionally, other tools might be useful: e.g., a **code execution tool** if we want an agent to actually run code tests (Anthropic API provides a code execution tool now). Since the user will use this with Python/Node, integrating testing frameworks or running scripts via Claude Code’s bash access can be done. We just must ensure any dangerous commands are either safely sandboxed or require manual approval, to maintain safety (Claude Code’s default is to ask permission for file writes, etc., unless overridden with a flag). Given our focus on research, the primary external tool is web search. If needed, we can use Node libraries like `serpapi` or Python’s `requests` to call search APIs outside of Claude as a fallback (then feed results into Claude). But leveraging Claude’s own tool-use is more seamless and lets it cite sources in responses.

* **Best Practice References:** We draw on known best practices from prior multi-agent systems: for instance, BabyAGI and AutoGPT demonstrated the importance of having a stopping criteria and refining loops. Our system will include heuristic limits (like max iterations for an agent solving a task, to avoid infinite loops). Microsoft’s Autogen framework shows how to script interactions – although we aren’t using it directly, their documentation can inspire our implementation of the interaction graph. The Anthropic engineering blog advises to *“explore, plan, code, commit”* as a workflow, emphasizing research and planning steps before execution – our Planner + Research strategy aligns with that. They also mention using the term “think” to give Claude more time for hard problems, which we’ll employ in prompts. And importantly, they note that Claude’s performance improves with iterative refinement and having clear targets or tests. So for critical outputs, we might add a final agent whose sole job is to **critique or test** the final answer against requirements (for instance, a “Reviewer” agent that checks if all parts of the user’s request are satisfied and no errors are present). This adds one more loop of feedback to improve quality, reminiscent of how one would use unit tests or acceptance criteria in coding – indeed, Anthropic suggests using subagents to verify implementations against tests. We will treat the user’s acceptance criteria as a “test” that the final output must pass.

In conclusion, this multi-agent system design for Claude Code is **feasible within Anthropic’s constraints** and oriented towards reliable, high-quality outcomes. We use Claude’s strengths (large context, tool use, long reasoning, multiple model sizes) and mitigate its limits (prompt quotas, potential hallucinations) through careful architecture: flexible agent roles, strong coordination via an orchestrator, persistent memory, research integration, and prompt management. By referencing both Anthropic’s latest capabilities (Claude 4, extended thinking, Claude Code SDK) and best practices from literature and existing agent systems, we ensure our design is both state-of-the-art and tailored for success under real-world conditions. The result will be a multi-agent setup that **adapts to each task, maximizes final answer quality, and remains robust and efficient** throughout its operation.

**Sources:**

* Anthropic Claude model and Claude Code announcements
* Anthropic engineering blog (Claude Code best practices)
* Anthropic documentation (Claude tool use, web search)
* Discussions on Claude usage limits
* Multi-agent systems research and examples
* Claude 4 improvements (memory, reliability)
* Claude Sonnet 4 for low hallucination Q\&A and Claude 3.5 Haiku for speed.
